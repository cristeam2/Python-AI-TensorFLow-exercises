{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 5> Logistic Regression in TensorFlow </font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between Linear regression and Logistic Regression?\n",
    "\n",
    "While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class in which an observed data point belongs. In order to provide estimate for classification, we need some sort of guidance on what would be the **most probable class** for that data point. For this, we use **Logistic Regression**.\n",
    "\n",
    "Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, __y__, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n",
    "\n",
    "Despite the name logistic regression, it is actually a **probabilistic classification model**. Logistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "ProbabilityOfaClass = y = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://ibm.box.com/shared/static/kgv9alcghmjcv97op4d6onkyxevk23b1.png' width='400', align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using logistic regression in TensorFlow \n",
    "\n",
    " We are going to exemplify how to perform a logistic regression using the data from the MINST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CristeamCaiolaPasqui\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# We imnport MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Now we import the dataset, in this case it is a set of images of handwritten digits. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0907 22:11:54.246982 31376 deprecation.py:323] From <ipython-input-2-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0907 22:11:54.247965 31376 deprecation.py:323] From C:\\Users\\CristeamCaiolaPasqui\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0907 22:11:54.252964 31376 deprecation.py:323] From C:\\Users\\CristeamCaiolaPasqui\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0907 22:12:10.825685 31376 deprecation.py:323] From C:\\Users\\CristeamCaiolaPasqui\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 22:12:11.647545 31376 deprecation.py:323] From C:\\Users\\CristeamCaiolaPasqui\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0907 22:12:11.650546 31376 deprecation.py:323] From C:\\Users\\CristeamCaiolaPasqui\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 22:12:15.154053 31376 deprecation.py:323] From C:\\Users\\CristeamCaiolaPasqui\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Now we define the training parameters: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define x and y. These placeholders will hold our data (both the features andn label matrices) and help pass them along to different parts of the algorithm. You can consider placeholders as empty shells into which we insert our data. We also need to give them shapes that correspond to the shape of our data. Later, we will insert data into these placeholders by feeding the placeholders the data via a feed_dict (feed dictionary):\n",
    "\n",
    "### ¿Why use Placeholders?  \n",
    "This feature of TensorFlow allows us to create an algorithm that accepts data and knows\n",
    "something about the shape of the data without knowing the amount of data going in. When we insert batches of data in training, we can easily adjust how many examples we train on in\n",
    "a single step without changing the entire algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model weights and bias\n",
    "\n",
    "Much like linear regression, we need a shared variable weight matrix for logistic regression.\n",
    "We initialize both W and b as tensors full of zeros. Since we are going to learn W and b, their\n",
    "initial value doesn't matter too much. These variables are the objects that define the\n",
    "structure of our regression model, and we can save them after they've been trained so that\n",
    "we can reuse them later.\n",
    "We define two TensorFlow variables as our parameters. These variables will hold the\n",
    "weights and biases of our logistic regression and they will be continually updated during\n",
    "training.\n",
    "Notice that W has a shape of [4, 3] because we want to multiply the 4-dimensional input\n",
    "vectors by it to produce 3-dimensional vectors of evidence for the difference classes. b has a\n",
    "shape of [3], so we can add it to the output. Moreover, unlike our placeholders (which are\n",
    "essentially empty shells waiting to be fed data), TensorFlow variables need to be initialized\n",
    "with values, say, with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression Model\n",
    "We now define our operations in order to properly run the logistic regression. Logistic regression is typically thought of as a single equation:\n",
    "\n",
    "$$\n",
    "ŷ =sigmoid(WX+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.nn.sigmoid(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The learning algorithm is how we search for the best weight vector ($ {\\bf w} $). This search is an\n",
    "optimization problem looking for the hypothesis that optimizes an error/cost measure.\n",
    "\n",
    " **What tells us how bad our model is?** \n",
    "\n",
    " The cost or loss of the model, so we want to minimize it. \n",
    "\n",
    " **What cost function are we going to use?** \n",
    "\n",
    " This time we are going to use cross entropy instead of the mean square error. \n",
    "\n",
    " **How are we going to minimize the cost function?** \n",
    "\n",
    " By gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_sum(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Now we create a session and initialize the variables: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorflow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize our weights and biases variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Initialize all tensorflow variables\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " And now we execute the training loop: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.004707754\n",
      "Epoch: 0002 cost= 0.004544954\n",
      "Epoch: 0003 cost= 0.004393467\n",
      "Epoch: 0004 cost= 0.004252071\n",
      "Epoch: 0005 cost= 0.004119888\n",
      "Epoch: 0006 cost= 0.003995963\n",
      "Epoch: 0007 cost= 0.003879538\n",
      "Epoch: 0008 cost= 0.003769990\n",
      "Epoch: 0009 cost= 0.003666629\n",
      "Epoch: 0010 cost= 0.003568991\n",
      "Epoch: 0011 cost= 0.003476609\n",
      "Epoch: 0012 cost= 0.003389099\n",
      "Epoch: 0013 cost= 0.003306043\n",
      "Epoch: 0014 cost= 0.003227068\n",
      "Epoch: 0015 cost= 0.003151936\n",
      "Epoch: 0016 cost= 0.003080362\n",
      "Epoch: 0017 cost= 0.003012018\n",
      "Epoch: 0018 cost= 0.002946831\n",
      "Epoch: 0019 cost= 0.002884414\n",
      "Epoch: 0020 cost= 0.002824765\n",
      "Epoch: 0021 cost= 0.002767571\n",
      "Epoch: 0022 cost= 0.002712772\n",
      "Epoch: 0023 cost= 0.002660159\n",
      "Epoch: 0024 cost= 0.002609614\n",
      "Epoch: 0025 cost= 0.002561007\n",
      "Optimization Finished!\n",
      "Accuracy: 0.60733336\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                      y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy for 3000 examples\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
